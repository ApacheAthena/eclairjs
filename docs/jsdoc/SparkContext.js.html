<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: SparkContext.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: SparkContext.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/*
 * Copyright 2015 IBM Corp.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


var initSparkContext = function (conf) {
    var logger = Logger.getLogger("SparkContext_js");
    if (typeof kernel !== 'undefined') {
        if (kernel.javaSparkContext() != null) {
            return kernel.javaSparkContext();
        } else {
            kernel.createSparkContext(Utils.unwrapObject(conf));
            return kernel.javaSparkContext();
        }
    }

    /*
     * Create a new JavaSparkContext from a conf
     *
     */
    var jvmSC = new org.apache.spark.api.java.JavaSparkContext(Utils.unwrapObject(conf));
    /*
     * add the jar for the cluster
     */
    var decodedPath = org.eclairjs.nashorn.Utils.jarLoc();
    var devEnvPath = "/target/classes/";
    var jarEnvPath = ".jar";
    logger.info("jar decodedPath = " + decodedPath);
    if (decodedPath.indexOf(devEnvPath,
            decodedPath.length - devEnvPath.length) !== -1) {
        /*
         * we are in the dev environment I hope...
         */
        jvmSC.addJar(decodedPath + "../eclairjs-nashorn-0.1.jar");
    } else if (decodedPath.indexOf(jarEnvPath,
            decodedPath.length - jarEnvPath.length) !== -1) {
        /*
         * We are running from a jar
         */
        jvmSC.addJar(decodedPath);
    }

    return jvmSC
};
/**
 *
 * @constructor
 * @classdesc A JavaScript-friendly version of SparkContext that returns RDDs
 * Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one.
 * This limitation may eventually be removed; see SPARK-2243 for more details.
 * @param {SparkConf} conf - a object specifying Spark parameters
 */
var SparkContext = function () {
    var jvmObj;
    this.logger = Logger.getLogger("SparkContext_js");
    if (arguments.length == 2) {
        var conf = new SparkConf()
        conf.setMaster(arguments[0])
        conf.setAppName(arguments[1])
        jvmObj = initSparkContext(conf)
    } else if (arguments.length==1 &amp;&amp;
        (arguments[0] instanceof org.apache.spark.api.java.JavaSparkContext))
    {
        jvmObj = arguments[0];
    } else {
        jvmObj = initSparkContext(arguments[0])
    }
    JavaWrapper.call(this, jvmObj);
    this.logger.debug(this.version());
};

SparkContext.prototype = Object.create(JavaWrapper.prototype);

//Set the "constructor" property to refer to SparkContext
SparkContext.prototype.constructor = SparkContext;

/**
 * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be
 * changed at runtime.
 * @returns {SparkConf}
 */
SparkContext.prototype.getConf = function () {
    var javaObject = this.getJavaObject().getConf();
    return new SparkConf(javaObject);
};


/**
 * @returns {string[]}
 */
SparkContext.prototype.jars = function () {
    return this.getJavaObject().jars();
};


/**
 * @returns {string[]}
 */
SparkContext.prototype.files = function () {
    return this.getJavaObject().files();
};


/**
 * @returns {string}
 */
SparkContext.prototype.master = function () {
    return this.getJavaObject().master();
};


/**
 * @returns {string}
 */
SparkContext.prototype.appName = function () {
    return this.getJavaObject().appName();
};


/**
 * @returns {boolean}
 */
SparkContext.prototype.isLocal = function () {
    return this.getJavaObject().isLocal();
};

/**
 * @returns {boolean}  true if context is stopped or in the midst of stopping.
 */
SparkContext.prototype.isStopped = function () {
    return this.getJavaObject().isStopped();
};


/**
 * @returns {SparkStatusTracker}
 */
SparkContext.prototype.statusTracker = function () {
    var javaObject = this.getJavaObject().statusTracker();
    return new SparkStatusTracker(javaObject);
};

/**
 * A unique identifier for the Spark application.
 * Its format depends on the scheduler implementation.
 * (i.e.
 *  in case of local spark app something like 'local-1433865536131'
 *  in case of YARN something like 'application_1433865536131_34483'
 * )
 * @returns {string}
 */
SparkContext.prototype.applicationId = function () {
    return this.getJavaObject().applicationId();
};


/**
 * @returns {string}
 */
SparkContext.prototype.applicationAttemptId = function () {
    return this.getJavaObject().applicationAttemptId();
};


/**
 * @param {string} logLevel  The desired log level as a string.
 * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
 */
SparkContext.prototype.setLogLevel = function (logLevel) {
    this.getJavaObject().setLogLevel(logLevel);
};


/**
 * initLocalProperties
 */
SparkContext.prototype.initLocalProperties = function () {
    this.getJavaObject().initLocalProperties();
};


/**
 * Set a local property that affects jobs submitted from this thread, such as the
 * Spark fair scheduler pool.
 * @param {string}
 * @param {string}
 */
SparkContext.prototype.setLocalProperty = function (key, value) {
    this.getJavaObject().setLocalProperty(key, value);
};


/**
 * Get a local property set in this thread, or null if it is missing. See
 * {@link setLocalProperty}.
 * @param {string}
 * @returns {string}
 */
SparkContext.prototype.getLocalProperty = function (key) {
    return this.getJavaObject().getLocalProperty(key);
};


/**
 * @param {string}
 */
SparkContext.prototype.setJobDescription = function (value) {
    this.getJavaObject().setJobDescription(value);
};


/**
 * Assigns a group ID to all the jobs started by this thread until the group ID is set to a
 * different value or cleared.
 *
 * Often, a unit of execution in an application consists of multiple Spark actions or jobs.
 * Application programmers can use this method to group all those jobs together and give a
 * group description. Once set, the Spark web UI will associate such jobs with this group.
 *
 * The application can also use {@link cancelJobGroup} to cancel all
 * running jobs in this group. For example,
 * @param {string}
 * @param {string}
 * @param {boolean}
 */
SparkContext.prototype.setJobGroup = function (groupId, description, interruptOnCancel) {
    this.getJavaObject().setJobGroup(groupId, description, interruptOnCancel);
};


/**
 * clearJobGroup
 */
SparkContext.prototype.clearJobGroup = function () {
    this.getJavaObject().clearJobGroup();
};


/**
 * Create an {@link Accumulable} shared variable of the given type, to which tasks can "add" values with add.
 * Only the master can access the accumuable's value.
 *
 * @param {object} initialValue
 * @param {AccumulableParam} param
 * @param {string} name of  the accumulator for display in Spark's web UI.
 * @returns {Accumulable}
 */
SparkContext.prototype.accumulable = function (initialValue, param, name) {
    return new Accumulable(initialValue, param, name);

};
/**
 * Create an {@link Accumulator}  variable, which tasks can "add" values to using the add method.
 * Only the master can access the accumulator's value.
 *
 * @param {int | float} initialValue
 * @param {string | AccumulableParam} [name] of  the accumulator for display in Spark's web UI. or param.  defaults to FloatAccumulatorParam
 * @param {AccumulableParam} [param]  defaults to FloatAccumulatorParam, use only if also specifying name
 * @returns {Accumulator}
 */
SparkContext.prototype.accumulator = function () {
    var initialValue = arguments[0];
    var name;
    var param = new FloatAccumulatorParam();
    this.logger.debug("accumulator " + initialValue);

    if (arguments[1]) {
        if (typeof arguments[1] === "string") {
            name = arguments[1];
            if (arguments[2]) {
                param = arguments[2];
            }
        } else {
            param = arguments[1];
        }
    }
    return new Accumulator(initialValue, param, name);

};
/**
 * Create an Accumulator integer variable, which tasks can "add" values to using the add method.
 * Only the master can access the accumulator's value.
 * @param {int} initialValue
 * @param {string} name of  the accumulator for display in Spark's web UI.
 * @returns {Accumulator}
 */
SparkContext.prototype.intAccumulator = function (initialValue, name) {
    return new Accumulator(initialValue, new IntAccumulatorParam(), name);

};
/**
 * Create an Accumulator float variable, which tasks can "add" values to using the add method.
 * Only the master can access the accumulator's value.
 * @param {float} initialValue
 * @param {string} name of  the accumulator for display in Spark's web UI.
 * @returns {Accumulator}
 */
SparkContext.prototype.floatAccumulator = function (initialValue, name) {
    return new Accumulator(initialValue, new FloatAccumulatorParam(), name);

};
/**
 *  Add a file to be downloaded with this Spark job on every node. The path passed can be either a local file,
 * a file in HDFS (or other Hadoop-supported filesystems), or an HTTP, HTTPS or FTP URI.
 * To access the file in Spark jobs, use SparkFiles.get(fileName) to find its download location.
 * @param {string} path - Path to the file
 */
SparkContext.prototype.addFile = function (path) {
    this.getJavaObject().addFile(path);
};
/**
 * Adds a JAR dependency for all tasks to be executed on this SparkContext in the future. The path passed can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), or an HTTP, HTTPS or FTP URI.
 * @param {string} path - Path to the jar
 */
SparkContext.prototype.addJar = function (path) {
    //public void addJar(java.lang.String path)
    this.getJavaObject().addJar(path);
};
/**
 * Broadcast a read-only variable to the cluster, returning a Broadcast object for reading it in distributed functions.
 * The variable will be sent to each cluster only once.
 * @param {object} value
 * @returns {Broadcast}
 */
SparkContext.prototype.broadcast = function (value) {
    return this.getJavaObject().broadcast(value);
};

/**
 * Distribute a local Scala collection to form an RDD.
 * @param {array} list
 * @param {integer} [numSlices]
 * @returns {RDD}
 */
SparkContext.prototype.parallelize = function (list, numSlices) {
    //public &lt;T> JavaRDD&lt;T> parallelize(java.util.List&lt;T> list, int numSlices)
    var list_uw = [];
    list.forEach(function (item) {
        list_uw.push(Utils.unwrapObject(item));
        //list_uw.push(Serialize.jsToJava(item));
    });
    if (numSlices) {
        return new RDD(this.getJavaObject().parallelize(list_uw, numSlices));
    } else {
        return new RDD(this.getJavaObject().parallelize(list_uw));
    }

};


/**
 * Distribute a local collection to form an RDD.
 * @param {array} list array of Tuple 2
 * @param {integer} numSlices
 * @returns {PairRDD}
 */
SparkContext.prototype.parallelizePairs = function (list, numSlices) {
    //public &lt;T> JavaRDD&lt;T> parallelize(java.util.List&lt;T> list, int numSlices)
    var list_uw = [];
    list.forEach(function (item) {
        list_uw.push(Utils.unwrapObject(item));
    });
    if (numSlices) {
        return new PairRDD(this.getJavaObject().parallelizePairs(list_uw, numSlices));
    } else {
        return new PairRDD(this.getJavaObject().parallelizePairs(list_uw));
    }

};
/**
 * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by
 * `step` every element.
 *
 * @note if we need to cache this RDD, we should make sure each partition does not exceed limit.
 *
 * @param {number} start  the start value.
 * @param {number} end  the end value.
 * @param {number} step  the incremental step
 * @param {number} numSlices  the partition number of the new RDD.
 * @returns {RDD}
 */
SparkContext.prototype.range = function (start, end, step, numSlices) {
    var javaObject = this.getJavaObject().range(start, end, step, numSlices);
    return new RDD(javaObject);
};


/**
 * Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI,
 * and return it as an RDD of Strings.
 * @param {string} path - path to file
 * @param {int} [minPartitions]
 * @returns {RDD}
 */
SparkContext.prototype.textFile = function (path, minPartitions) {
    if (minPartitions) {
        return new RDD(this.getJavaObject().textFile(path, minPartitions));
    } else {
        return new RDD(this.getJavaObject().textFile(path));
    }

};


/**
 * Read a directory of text files from HDFS, a local file system (available on all nodes), or any
 * Hadoop-supported file system URI. Each file is read as a single record and returned in a
 * key-value pair, where the key is the path of each file, the value is the content of each file.
 *
 * &lt;p> For example, if you have the following files:
 * @example
 *   hdfs://a-hdfs-path/part-00000
 *   hdfs://a-hdfs-path/part-00001
 *   ...
 *   hdfs://a-hdfs-path/part-nnnnn
 *
 *
 * Do `var rdd = sparkContext.wholeTextFile("hdfs://a-hdfs-path")`,
 *
 * &lt;p> then `rdd` contains
 * @example
 *   (a-hdfs-path/part-00000, its content)
 *   (a-hdfs-path/part-00001, its content)
 *   ...
 *   (a-hdfs-path/part-nnnnn, its content)
 *
 *
 * @note Small files are preferred, large file is also allowable, but may cause bad performance.
 * @note On some filesystems, `.../path/&amp;#42;` can be a more efficient way to read all files
 *       in a directory rather than `.../path/` or `.../path`
 *
 * @param {string} path  Directory to the input data files, the path can be comma separated paths as the
 *             list of inputs.
 * @param {number} minPartitions  A suggestion value of the minimal splitting number for input data.
 * @returns {RDD}
 */
SparkContext.prototype.wholeTextFiles = function (path, minPartitions) {
    var javaObject = this.getJavaObject().wholeTextFiles(path, minPartitions);
    return new RDD(javaObject);
};

/**
 * Set the directory under which RDDs are going to be checkpointed.
 * The directory must be a HDFS path if running on a cluster.
 * @param {string} dir
 */
SparkContext.prototype.setCheckpointDir = function (dir) {
    this.getJavaObject().setCheckpointDir(dir);
};

/**
 * Shut down the SparkContext.
 */
SparkContext.prototype.stop = function () {
    this.getJavaObject().stop();
};

/**
 * The version of EclairJS and Spark on which this application is running.
 * @returns {string}
 */
SparkContext.prototype.version = function () {
    var javaVersion = java.lang.System.getProperty("java.version");
    var jv = javaVersion.split(".");
    var wrongJavaVersionString = "Java 1.8.0_60 or greater required for EclairJS";
    var wrongSparkVersionString = "Spark 1.5.1 or greater required for EclairJS";
    if (jv[0] &lt; 2) {
        if (jv[0] == 1) {
            if (jv[1] &lt; 8) {
                throw wrongJavaVersionString;
            } else {
                if (jv[1] == 8) {
                    // we are at 1.8
                    var f = jv[2]
                    var fix = f.split("_");
                    if ((fix[0] &lt; 1) &amp;&amp; (fix[1] &lt; 60)) {
                        // less than 1.8.0_60
                        throw wrongJavaVersionString;
                    }
                } else {
                    // 1.9 or greater
                }
            }
        } else {
            throw wrongJavaVersionString;
        }

    } else {
        // versions is 2.0 or greater
    }
    var sparkVersion = this.getJavaObject().version();
    var sv = sparkVersion.split(".");
    if (sv[0] &lt; 2) {
        if (sv[0] == 1) {
            if (sv[1] &lt; 5) {
                throw wrongSparkVersionString;
            } else {
                if (sv[1] == 5) {
                    // we are at 1.5
                    if (sv[2] &lt; 1) {
                        // less than 1.5.1
                        throw wrongSparkVersionString;
                    }
                } else {
                    // 1.5 or greater
                }
            }
        } else {
            throw wrongSparkVersionString;
        }

    } else {
        // versions is 2.0 or greater
    }
    return "EclairJS-nashorn 0.1 Spark " + sparkVersion;
};
</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="Accumulable.html">Accumulable</a></li><li><a href="AccumulableParam.html">AccumulableParam</a></li><li><a href="Accumulator.html">Accumulator</a></li><li><a href="ALS.html">ALS</a></li><li><a href="ArrayType.html">ArrayType</a></li><li><a href="AssociationRules.html">AssociationRules</a></li><li><a href="BinaryClassificationMetrics.html">BinaryClassificationMetrics</a></li><li><a href="BinaryType.html">BinaryType</a></li><li><a href="BisectingKMeans.html">BisectingKMeans</a></li><li><a href="BisectingKMeansModel.html">BisectingKMeansModel</a></li><li><a href="BooleanType.html">BooleanType</a></li><li><a href="BoostingStrategy.html">BoostingStrategy</a></li><li><a href="CalendarIntervalType.html">CalendarIntervalType</a></li><li><a href="Column.html">Column</a></li><li><a href="DataFrame.html">DataFrame</a></li><li><a href="DataFrameNaFunctions.html">DataFrameNaFunctions</a></li><li><a href="DataFrameReader.html">DataFrameReader</a></li><li><a href="DataFrameStatFunctions.html">DataFrameStatFunctions</a></li><li><a href="DataFrameWriter.html">DataFrameWriter</a></li><li><a href="DataType.html">DataType</a></li><li><a href="DataTypes.html">DataTypes</a></li><li><a href="DateType.html">DateType</a></li><li><a href="DecisionTree.html">DecisionTree</a></li><li><a href="DecisionTreeModel.html">DecisionTreeModel</a></li><li><a href="DenseMatrix.html">DenseMatrix</a></li><li><a href="DenseVector.html">DenseVector</a></li><li><a href="DistributedLDAModel.html">DistributedLDAModel</a></li><li><a href="DistributedMatrix.html">DistributedMatrix</a></li><li><a href="DoubleType.html">DoubleType</a></li><li><a href="DStream.html">DStream</a></li><li><a href="Duration.html">Duration</a></li><li><a href="FloatAccumulatorParam.html">FloatAccumulatorParam</a></li><li><a href="FloatRDD.html">FloatRDD</a></li><li><a href="FloatType.html">FloatType</a></li><li><a href="FPGrowth.html">FPGrowth</a></li><li><a href="FPGrowthModel.html">FPGrowthModel</a></li><li><a href="FreqItemset.html">FreqItemset</a></li><li><a href="functions.html">functions</a></li><li><a href="FutureAction.html">FutureAction</a></li><li><a href="GeneralizedLinearModel.html">GeneralizedLinearModel</a></li><li><a href="Gradient.html">Gradient</a></li><li><a href="GradientBoostedTrees.html">GradientBoostedTrees</a></li><li><a href="GradientBoostedTreesModel.html">GradientBoostedTreesModel</a></li><li><a href="GroupedData.html">GroupedData</a></li><li><a href="HashPartitioner.html">HashPartitioner</a></li><li><a href="IntAccumulatorParam.html">IntAccumulatorParam</a></li><li><a href="IntegerType.html">IntegerType</a></li><li><a href="IsotonicRegression.html">IsotonicRegression</a></li><li><a href="IsotonicRegressionModel.html">IsotonicRegressionModel</a></li><li><a href="KMeans.html">KMeans</a></li><li><a href="KMeansModel.html">KMeansModel</a></li><li><a href="LabeledPoint.html">LabeledPoint</a></li><li><a href="LBFGS.html">LBFGS</a></li><li><a href="LDA.html">LDA</a></li><li><a href="LDAModel.html">LDAModel</a></li><li><a href="LinearRegressionModel.html">LinearRegressionModel</a></li><li><a href="LinearRegressionWithSGD.html">LinearRegressionWithSGD</a></li><li><a href="List.html">List</a></li><li><a href="LocalLDAModel.html">LocalLDAModel</a></li><li><a href="Logger.html">Logger</a></li><li><a href="LogisticGradient.html">LogisticGradient</a></li><li><a href="LogisticRegressionModel.html">LogisticRegressionModel</a></li><li><a href="LogisticRegressionWithLBFGS.html">LogisticRegressionWithLBFGS</a></li><li><a href="LogisticRegressionWithSGD.html">LogisticRegressionWithSGD</a></li><li><a href="Loss.html">Loss</a></li><li><a href="MapType.html">MapType</a></li><li><a href="Matrix.html">Matrix</a></li><li><a href="MatrixFactorizationModel.html">MatrixFactorizationModel</a></li><li><a href="Metadata.html">Metadata</a></li><li><a href="MLWord2Vec.html">MLWord2Vec</a></li><li><a href="MLWord2VecModel.html">MLWord2VecModel</a></li><li><a href="MulticlassMetrics.html">MulticlassMetrics</a></li><li><a href="MultilabelMetrics.html">MultilabelMetrics</a></li><li><a href="NaiveBayes.html">NaiveBayes</a></li><li><a href="NaiveBayesModel.html">NaiveBayesModel</a></li><li><a href="NullType.html">NullType</a></li><li><a href="NumericType.html">NumericType</a></li><li><a href="PairDStream.html">PairDStream</a></li><li><a href="PairRDD.html">PairRDD</a></li><li><a href="PartialResult.html">PartialResult</a></li><li><a href="Partitioner.html">Partitioner</a></li><li><a href="PowerIterationClustering.html">PowerIterationClustering</a></li><li><a href="PowerIterationClusteringAssignment.html">PowerIterationClusteringAssignment</a></li><li><a href="PowerIterationClusteringModel.html">PowerIterationClusteringModel</a></li><li><a href="PrefixSpan.html">PrefixSpan</a></li><li><a href="PrefixSpanFreqSequence.html">PrefixSpanFreqSequence</a></li><li><a href="PrefixSpanModel.html">PrefixSpanModel</a></li><li><a href="RandomForest.html">RandomForest</a></li><li><a href="RandomForestModel.html">RandomForestModel</a></li><li><a href="RangePartitioner.html">RangePartitioner</a></li><li><a href="RankingMetrics.html">RankingMetrics</a></li><li><a href="Rating.html">Rating</a></li><li><a href="RDD.html">RDD</a></li><li><a href="RegressionMetrics.html">RegressionMetrics</a></li><li><a href="Row.html">Row</a></li><li><a href="RowFactory.html">RowFactory</a></li><li><a href="RowMatrix.html">RowMatrix</a></li><li><a href="Rule.html">Rule</a></li><li><a href="SparkConf.html">SparkConf</a></li><li><a href="SparkContext.html">SparkContext</a></li><li><a href="SparkFiles.html">SparkFiles</a></li><li><a href="SparkStatusTracker.html">SparkStatusTracker</a></li><li><a href="SparseMatrix.html">SparseMatrix</a></li><li><a href="SparseVector.html">SparseVector</a></li><li><a href="SQLContext.html">SQLContext</a></li><li><a href="SQLContext.QueryExecution.html">QueryExecution</a></li><li><a href="SQLContext.SparkPlanner.html">SparkPlanner</a></li><li><a href="SQLContext.SQLSession.html">SQLSession</a></li><li><a href="SqlDate.html">SqlDate</a></li><li><a href="SqlTimestamp.html">SqlTimestamp</a></li><li><a href="SquaredL2Updater.html">SquaredL2Updater</a></li><li><a href="StorageLevel.html">StorageLevel</a></li><li><a href="Strategy.html">Strategy</a></li><li><a href="StreamingContext.html">StreamingContext</a></li><li><a href="StringType.html">StringType</a></li><li><a href="StructField.html">StructField</a></li><li><a href="StructType.html">StructType</a></li><li><a href="Time.html">Time</a></li><li><a href="TimestampType.html">TimestampType</a></li><li><a href="Tuple.html">Tuple</a></li><li><a href="Updater.html">Updater</a></li><li><a href="Vector.html">Vector</a></li><li><a href="Vectors.html">Vectors</a></li><li><a href="VectorUDT.html">VectorUDT</a></li><li><a href="Word2Vec.html">Word2Vec</a></li><li><a href="Word2VecModel.html">Word2VecModel</a></li></ul><h3>Interfaces</h3><ul><li><a href="ClassificationModel.html">ClassificationModel</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc3/jsdoc">JSDoc 3.3.2</a> on Tue Mar 22 2016 12:14:23 GMT-0400 (EDT)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
